{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ValueInvestor2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ValueInvestor"
      ],
      "metadata": {
        "id": "JsNOKKlz5W3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal(s):**\n",
        "\n",
        "Predict stock price valuations on a daily, weekly and monthly basis. Recommend BUY, HOLD, SELL decisions. Maximize capital returns, minimize losses. Ideally a loss should never happen. Minimize HOLD period.\n",
        "\n",
        "**Data Description:**\n",
        "\n",
        "A set of portfolio companies trading data from emerging markets including 2020 Q1-Q2-Q3-Q4 2021 Q1 stock prices. Each company stock is provided in different sheets. Each market's operating days varies based on the country of the company and the market the stocks are exchanged.\n",
        "\n",
        "**Success Metrics:**\n",
        "\n",
        "Evaluate on the basis of capital returns. Use Bollinger Bands to measure your systems effectiveness."
      ],
      "metadata": {
        "id": "z4KwyrJi5aoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import math\n",
        "import imblearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from hyperopt import tpe\n",
        "import seaborn as sns\n",
        "from tpot import TPOTClassifier\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import date\n",
        "from skopt import BayesSearchCV\n",
        "from fbprophet import Prophet\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from hpsklearn import HyperoptEstimator\n",
        "from hpsklearn import any_classifier\n",
        "from hpsklearn import any_preprocessing\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from collections import Counter\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import  GRU\n",
        "from pmdarima.arima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers.recurrent_v2 import GRU\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
        "from fbprophet.plot import add_changepoints_to_plot, plot_cross_validation_metric\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose"
      ],
      "metadata": {
        "id": "L4e4XEwS8HOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open data\n",
        "dfColombia = pd.read_excel('2020Q1Q2Q3Q4-2021Q1.xlsx', sheet_name = 'Colombia - Cementos Argos SA (C') \n",
        "dfColombia"
      ],
      "metadata": {
        "id": "-nerPiNL5t4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataframe by row\n",
        "df_1 = dfColombia.iloc[:303,:]\n",
        "df_2 = dfColombia.iloc[303:,:]"
      ],
      "metadata": {
        "id": "349ogKxS540h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace - characters\n",
        "df_1['Change %'] = df_1['Change %'].astype('string')\n",
        "df_1['Vol.'] = df_1['Vol.'].astype('string')"
      ],
      "metadata": {
        "id": "ZI28BWNA5757"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1['Change %'] = df_1['Change %'].str.replace(\"%\", \" \")\n",
        "#replace - characters\n",
        "df_1['Vol.'] = df_1['Vol.'].str.replace(\"M\", \" \")\n",
        "#replace - characters\n",
        "df_1['Vol.'] = df_1['Vol.'].str.replace(\"K\", \" \")\n",
        "df_1['Vol.'] = df_1['Vol.'].astype(float)\n",
        "df_1['Price'] = df_1['Price'].astype(float)\n",
        "df_1['Open'] = df_1['Open'].astype(float)\n",
        "df_1['High'] = df_1['High'].astype(float)\n",
        "df_1['Low'] = df_1['Low'].astype(float)\n",
        "df_1['Change %'] = df_1['Change %'].astype(float)\n",
        "#datetime\n",
        "df_1['Date'] = pd.to_datetime(df_1['Date'])"
      ],
      "metadata": {
        "id": "-iTYwQN-5-gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter df by 2020\n",
        "filtered_df = df_1[df_1[\"Date\"].isin(pd.date_range('2020-2-1', '2020-12-31'))]\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "xegb0uK66CIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set index\n",
        "current_df = filtered_df.set_index('Date')"
      ],
      "metadata": {
        "id": "4RVEoYUo6J4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sma\n",
        "def get_sma(prices, rate):\n",
        "    return prices.rolling(rate).mean()"
      ],
      "metadata": {
        "id": "DBYzpzZw6T5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closing_prices = current_df['Price'] # Use only closing prices"
      ],
      "metadata": {
        "id": "4nI5mW_Y6W5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sma = get_sma(closing_prices, 20) # Get 20 day SMA"
      ],
      "metadata": {
        "id": "-Dz7D88T6Zhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbol = ' SA'"
      ],
      "metadata": {
        "id": "YER2uyNC6cX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bollinger bands\n",
        "def get_bollinger_bands(prices, sma, rate=20):\n",
        "    sma = get_sma(prices, rate)\n",
        "    std = prices.rolling(rate).std()\n",
        "    bollinger_up = sma + std * 2 # Calculate top band\n",
        "    bollinger_down = sma - std * 2 # Calculate bottom band\n",
        "    return bollinger_up, bollinger_down"
      ],
      "metadata": {
        "id": "xYdW_58p6dH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bollinger_up, bollinger_down = get_bollinger_bands(closing_prices, sma)"
      ],
      "metadata": {
        "id": "EL2znAzl6ka1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_df['sma_20'] = get_sma(current_df['Price'], 20)\n",
        "current_df.tail()"
      ],
      "metadata": {
        "id": "j9ZHB5-U6pbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bollinger bands\n",
        "current_df['upper_bb'], current_df['lower_bb'] = get_bollinger_bands(current_df['Price'], current_df['sma_20'], 20)\n",
        "current_df.tail()"
      ],
      "metadata": {
        "id": "IcVUGsud6qli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#crosspoints\n",
        "buyers = current_df[current_df['Price'] <= current_df['lower_bb']]\n",
        "sellers = current_df[current_df['Price'] >= current_df['upper_bb']]"
      ],
      "metadata": {
        "id": "mqWwdtg76xLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5gG6mpg3Eq1"
      },
      "outputs": [],
      "source": [
        "pip install tpot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tpot Looks for the best combination of models and tunings"
      ],
      "metadata": {
        "id": "Tu6G4V393VLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modeldf = current_df.drop(['actions','lower_bb','upper_bb','sma_20'], axis =1)"
      ],
      "metadata": {
        "id": "0XRrlGQ-77LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(Yxis)\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "xvCfS9wQ8B33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the dataset\n",
        "scaler = StandardScaler()\n",
        "current_df_norm = scaler.fit_transform(modeldf)\n",
        "Xaxis = np.array(current_df_norm)\n",
        "Yxis = np.array(current_df['actions'])\n",
        "#balance the labels\n",
        "\n",
        "oversample = SMOTE()\n",
        "Xaxis, Yxis = oversample.fit_resample(Xaxis, Yxis)\n"
      ],
      "metadata": {
        "id": "YVxLfhZA7-6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "winning_pipes = []\n",
        "tpot = TPOTClassifier(verbosity=3, \n",
        "                      scoring=\"balanced_accuracy\", \n",
        "                      random_state=23, \n",
        "                      periodic_checkpoint_folder=\"tpot_mnst1.txt\", \n",
        "                      n_jobs=-1, \n",
        "                      generations=10, \n",
        "                      population_size=100)\n",
        "# run three iterations and time them\n",
        "for x in range(3):\n",
        "    \n",
        "    tpot.fit(Xaxis, Yxis)\n",
        "    winning_pipes.append(tpot.fitted_pipeline_)\n",
        "    scores.append(tpot.score(Xaxis, Yxis))\n",
        "print('Scores:', scores)  \n",
        "print('Winning pipelines:', winning_pipes)"
      ],
      "metadata": {
        "id": "0jIBLXLn3XsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Optimization finds the best optimized parameteres for a given model."
      ],
      "metadata": {
        "id": "2XNA-cpb3n3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = BayesSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    {\n",
        "      'bootstrap': [True, False],\n",
        "               'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
        "               'max_features': ['auto', 'sqrt'],\n",
        "               'min_samples_leaf': [1, 2, 4],\n",
        "               'min_samples_split': [2, 5, 10],\n",
        "               'n_estimators': [130, 180, 230],\n",
        "    },\n",
        "    n_iter=32,\n",
        "    cv=3\n",
        ")\n",
        "\n",
        "opt.fit(Xaxis,Yxis)\n",
        "\n",
        "print(\"val. score: %s\" % opt.best_score_)\n",
        "print(\"test score: %s\" % opt.score(Xaxis,Yxis))"
      ],
      "metadata": {
        "id": "xqqZjVG13qy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-optimize"
      ],
      "metadata": {
        "id": "xE5cC3Ch3fxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper optimization is another good technique that finds the best optimized parameteres for a given model."
      ],
      "metadata": {
        "id": "V5Un55u23xx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hyper optimization is another good technique that finds the best optimized parameteres for a given model."
      ],
      "metadata": {
        "id": "trjfSsp43wb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define search\n",
        "modelopt = HyperoptEstimator()\n",
        "\n",
        "\n",
        "# perform the search\n",
        "modelopt.fit(Xaxis,Yxis)\n",
        "\n",
        "# summarize performance\n",
        "acc = modelopt.score(Xaxis,Yxis)\n",
        "print(\"Accuracy: %.3f\" % acc)\n",
        "# summarize the best model\n",
        "print(modelopt.best_model())"
      ],
      "metadata": {
        "id": "RjtYsCRZ32PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression and Classification Model combination."
      ],
      "metadata": {
        "id": "rJGI7GpE36An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#axis's\n",
        "xOpen = current_df.Open.values\n",
        "yPrice = current_df.Price.values\n",
        "y_class = current_df.actions"
      ],
      "metadata": {
        "id": "oFFe8G3039Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting\n",
        "train_X, test_X = xOpen[0:int(0.725*(len(xOpen)))], xOpen[int(0.725*(len(xOpen))):]\n",
        "train_Y, test_Y = yPrice[0:int(0.725*(len(yPrice)))], yPrice[int(0.725*(len(yPrice))):]\n",
        "y_train_class, y_test_class = y_class[0:int(0.725*(len(y_class)))], y_class[int(0.725*(len(y_class))):]"
      ],
      "metadata": {
        "id": "j7qtPzhq4AMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#number of features and number of classes\n",
        "n_features = train_X.shape[1]\n",
        "n_class = len(unique(y_class))\n"
      ],
      "metadata": {
        "id": "RrFvW8OH4A85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "visible = Input(shape=(n_features,))\n",
        "hidden1 = Dense(160, activation='relu', kernel_initializer='he_normal')(visible)\n",
        "hidden2 = Dense(160, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
        "# regression output\n",
        "out_reg = Dense(1, activation='linear')(hidden2)\n",
        "# classification output\n",
        "out_clas = Dense(n_class, activation='softmax')(hidden2)\n",
        "# define model\n",
        "modelmix = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
        "# compile the keras model\n",
        "modelmix.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "modelmix.fit(train_X, [train_Y,y_train_class], epochs=150, batch_size=1, verbose=2)\n",
        "# make predictions on test set\n",
        "yhat1, yhat2 = modelmix.predict(test_X)\n",
        "# calculate error for regression model\n",
        "error = mean_absolute_error(test_Y, yhat1)\n",
        "print('MAE: %.3f' % error)\n",
        "# evaluate accuracy for classification model\n",
        "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test_class, yhat2)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "7E--cW0S4EiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results\n",
        "plt.plot(yPrice[:61], color = 'black', label = ' Stock Price')\n",
        "plt.plot(yhat1, color = 'green', label = 'Predicted  Stock Price')\n",
        "plt.plot(yhat2, color = 'red', label = 'Labels')\n",
        "plt.title(' Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel(' Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ox6S97by4HSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Classification."
      ],
      "metadata": {
        "id": "2EgV9AQt4MdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x shape\n",
        "x_totrain = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))"
      ],
      "metadata": {
        "id": "rVKahQey4NX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=3, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(n_class, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "modelkeras = make_model(input_shape=x_totrain.shape[1:])\n",
        "keras.utils.plot_model(modelkeras, show_shapes=True)"
      ],
      "metadata": {
        "id": "tHMZw4LC4P6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "epochs = 50\n",
        "batch_size = 4\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.001)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1),\n",
        "]\n",
        "modelkeras.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "history = modelkeras.fit(\n",
        "    x_totrain,\n",
        "    y_train_class,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,"
      ],
      "metadata": {
        "id": "4rtjd7rg4VXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x test reshape\n",
        "retestxx = test_X.reshape(61,1)"
      ],
      "metadata": {
        "id": "Pe8q9G9s4ZVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_totest = retestxx.reshape((retestxx.shape[0], retestxx.shape[1], 1))"
      ],
      "metadata": {
        "id": "F9GOzUcc4aTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x test predict\n",
        "yhatkeras = modelkeras.predict(x_totest)"
      ],
      "metadata": {
        "id": "U3v_JNvf4e69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results\n",
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "9SHJeY5G4jUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daily returns based on a percantage change in a price by row."
      ],
      "metadata": {
        "id": "3R7JbhDM4mce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_df['daily_returns'] = current_df['Price'].pct_change()"
      ],
      "metadata": {
        "id": "__hcNdj14tai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count missing values\n",
        "current_df['daily_returns'].isna().sum()"
      ],
      "metadata": {
        "id": "hluMuLCw4zJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove missing values\n",
        "current_df['daily_returns'].fillna(int(current_df['daily_returns'].mean()), inplace=True)"
      ],
      "metadata": {
        "id": "hzL4SrY942Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
        "ax1.plot(current_df['daily_returns'])\n",
        "ax1.set_xlabel(\"Date\")\n",
        "ax1.set_ylabel(\"Percent\")\n",
        "ax1.set_title(\"daily returns data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSRkYhNJ45Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Combination models to predict labels based on daily returns percantage change."
      ],
      "metadata": {
        "id": "ZmK7LBlt47Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# axis's\n",
        "xPrice = current_df.Price.values\n",
        "yreturn = current_df.daily_returns.values\n",
        "y_class = current_df.actions\n",
        "#split\n",
        "trainxxprice, testxxprice = xPrice[0:int(0.725*(len(xPrice)))], xPrice[int(0.725*(len(xPrice))):]\n",
        "trainyreturns, testyreturns = yreturn[0:int(0.725*(len(yreturn)))], yreturn[int(0.725*(len(yreturn))):]\n",
        "y_train_class, y_test_class = y_class[0:int(0.725*(len(y_class)))], y_class[int(0.725*(len(y_class))):]\n",
        "trainxxprice = trainxxprice.reshape(160,1)\n",
        "n_features = trainxxprice.shape[1]\n",
        "n_class = len(unique(y_class))"
      ],
      "metadata": {
        "id": "xnnGDa2w4-BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "visible = Input(shape=(n_features,))\n",
        "hidden1 = Dense(160, activation='relu', kernel_initializer='he_normal')(visible)\n",
        "hidden2 = Dense(160, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
        "# regression output\n",
        "out_reg = Dense(1, activation='linear')(hidden2)\n",
        "# classification output\n",
        "out_clas = Dense(n_class, activation='softmax')(hidden2)\n",
        "# define model\n",
        "modeltest = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
        "# compile the keras model\n",
        "modeltest.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "modeltest.fit(trainxxprice, [trainyreturns,y_train_class], epochs=150, batch_size=1, verbose=2)\n",
        "# make predictions on test set\n",
        "yhat1test, yhat2test = modeltest.predict(testxxprice)\n",
        "# calculate error for regression model\n",
        "error = mean_absolute_error(testyreturns, yhat1test)\n",
        "print('MAE: %.3f' % error)\n",
        "# evaluate accuracy for classification model\n",
        "yhat2 = argmax(yhat2test, axis=-1).astype('int')\n",
        "acc = accuracy_score(testyreturns, yhat2test)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "metadata": {
        "id": "7k9aF8Ir5Bn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results\n",
        "plt.plot(yreturn[:61], color = 'black', label = ' Returns')\n",
        "plt.plot(yhat1test, color = 'green', label = 'Predicted Returns')\n",
        "plt.plot(yhat2test, color = 'red', label = 'Labels')\n",
        "plt.title(' Returns Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel(' Returns')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n5EaXX_M5ESl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "epochs = 50\n",
        "batch_size = 4\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.0001)\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1),\n",
        "]\n",
        "modelkeras.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "historyret = modelkeras.fit(\n",
        "    trainxxprice,\n",
        "    y_train_class,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "cM2GCYaX5Hvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict\n",
        "yhatkeras = modelkeras.predict(x_totest)"
      ],
      "metadata": {
        "id": "r8_WEm7K5LSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot results\n",
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(historyret.history[metric])\n",
        "plt.plot(historyret.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "EdgZb8-Y5ObY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kZzk4XzM5ToE"
      }
    }
  ]
}